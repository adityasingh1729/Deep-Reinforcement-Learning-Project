{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Tic-Tac-Toe using TD(λ)\n",
    "In this assignment you will build an RL agent capable of playing Tic-Tac-Toe using TD(λ) algorithm and the environment simulated by you in first week.\n",
    "\n",
    "First of all copy the environment simulated by you from the first week below.\n",
    "- Note that you should also return the state of the board each time you call act function, ideally the state should be stored in a numpy array for faster implementation\n",
    "- The only input the function can take is via its own arguments and not input() function.\n",
    "\n",
    "The ideal TicTacToe environment:\n",
    "- Will take N, the size of board as an argument in its constructor.\n",
    "- Will have act function taking a single argument representing the action taken (preferably int type) and return the state of board (preferably numpy array), reward signal (float) and bool value \"done\" which is true if the game is over else false.\n",
    "- Will have reset function which resets the board and starts a new game.\n",
    "- Will give reward signal as 1 if 'X' won and -1 if 'O' won (or vice-versa) and 0 if its a draw.\n",
    "- Will take alternate calls of act function as the moves of one player.\n",
    "\n",
    "For example:\n",
    "```html\n",
    "env.reset()\n",
    "Returns ==> (array([[0., 0., 0.],[0., 0., 0.],[0., 0., 0.]]), 0, False)\n",
    "                | | | |\n",
    "        Board:  | | | |\n",
    "                | | | |\n",
    "\n",
    "env.act(4)\n",
    "Returns ==> (array([[0., 0., 0.],[0., 1.0, 0.],[0., 0., 0.]]), 0, False)\n",
    "                | | | |\n",
    "        Board:  | |X| |\n",
    "                | | | |\n",
    "\n",
    "env.act(0)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[0., 0., 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                | | | |\n",
    "\n",
    "env.act(7)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[0., 1.0, 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                | |X| |\n",
    "\n",
    "env.act(6)\n",
    "Returns ==> (array([[-1.0, 0., 0.],[0., 1.0, 0.],[-1.0, 1.0, 0.]]), 0, False)\n",
    "                |O| | |\n",
    "        Board:  | |X| |\n",
    "                |O|X| |\n",
    "\n",
    "env.act(2)\n",
    "Returns ==> (array([[-1.0, 1.0, 0.],[0., 1.0, 0.],[-1.0, 1.0, 0.]]), 1, True)\n",
    "                |O|X| |\n",
    "        Board:  | |X| |\n",
    "                |O|X| |\n",
    "\n",
    "\n",
    "```\n",
    "<hr>\n",
    "\n",
    "Note : You can change your TicTacToe environment code before using it here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any necessary libraries here\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your TicTacToe environment class comes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, n : int = 3):\n",
    "        self.n = n\n",
    "        self.grid = np.full((n, n), 0)\n",
    "        self.currMove = 1\n",
    "        self.moveNum = 0\n",
    "        self.xMove = None\n",
    "        self.yMove = None\n",
    "\n",
    "\n",
    "    # Given below is the preferable structure of act function\n",
    "    def act(self, action : int) -> tuple:     # Returns tuple of types (np.ndarray, int, bool)\n",
    "        self.action = action\n",
    "        self.xMove = action // self.n\n",
    "        self.yMove = action % self.n\n",
    "        maxMove = self.n ** 2\n",
    "        if action < 0 or action >= maxMove or self.grid[self.xMove][self.yMove] != 0:\n",
    "            print(\"Wrong Move, try again!\")\n",
    "            return self.grid, 0, False\n",
    "        self.moveNum += 1\n",
    "        self.grid[self.xMove][self.yMove] = self.currMove\n",
    "        if self._checkWin() == True:\n",
    "            if self.currMove == 1:\n",
    "                return self.grid, 1, True\n",
    "            return self.grid, -1, True\n",
    "        if self.moveNum == maxMove:\n",
    "            return self.grid, 0, True\n",
    "        self._changeMove()\n",
    "        return self.grid, 0, False\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = np.full((self.n, self.n), 0)\n",
    "        self.currMove = 1\n",
    "        return self.grid, 0, False\n",
    "\n",
    "    def _checkWin(self):\n",
    "    # Check rows\n",
    "        for row in self.grid:\n",
    "            if np.all(row == self.currMove):\n",
    "                return True\n",
    "\n",
    "        # Check columns\n",
    "        for col in self.grid.T:\n",
    "            if np.all(col == self.currMove):\n",
    "                return True\n",
    "\n",
    "        # Check diagonals\n",
    "        diag1 = np.diag(self.grid)\n",
    "        diag2 = np.diag(np.fliplr(self.grid))\n",
    "        if np.all(diag1 == self.currMove) or np.all(diag2 == self.currMove):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def _changeMove(self):\n",
    "        if self.currMove == 1:\n",
    "            self.currMove = -1\n",
    "        else:\n",
    "            self.currMove = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes the agent class which\n",
    "- Uses TD(λ) algorithm to find the optimal policies for each state\n",
    "- Stores the calculated optimal policies as a .npy file for later use\n",
    "- Calculates the average return of the itself against a random player (makes random moves on its chance) periodically during training and plot it (for example if total training iterations is 10000, then calculate average return after each 500 steps, also for average return you should calculate return atleast 5 times and then take average)\n",
    "- You can make additional functions\n",
    "\n",
    "You can store all the encountered states in a numpy array (which will have 3 dims) and then store corresponding values for that particulare state in another array (will have 1 dims) and then you can store all these arrays in a .npy file for future use, so that you don't have to train the model each time you want to play TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the constructors and train() function can have any arguments you need\n",
    "class TicTacToeAgent:\n",
    "    def __init__(self, tic_tac_toe, alpha=0.1, gamma=0.9, epsilon=0.1, lambda_=0.5):\n",
    "        self.tic_tac_toe = tic_tac_toe\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "        # self.state_values = {}\n",
    "        self.state_values = np.load('stored_data.npy', allow_pickle=True).item()\n",
    "        self.eligibility_traces = {}\n",
    "\n",
    "    def train(self, episodes=100000, evaluate_every=500):\n",
    "        returns = []\n",
    "        for episode in range(episodes):\n",
    "            self.eligibility_traces.clear()\n",
    "            state = self.tic_tac_toe.reset()[0]\n",
    "            done = False\n",
    "            total_return = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                if action == None:\n",
    "                    done = True\n",
    "                    break\n",
    "                next_state, reward, done = self.tic_tac_toe.act(action)\n",
    "                total_return += reward\n",
    "\n",
    "                td_target = reward if done else reward + self.gamma * self.get_state_value(next_state)\n",
    "                td_error = td_target - self.get_state_value(state)\n",
    "\n",
    "                self.update_state_value(state, td_error)\n",
    "                state = next_state\n",
    "\n",
    "            returns.append(total_return)\n",
    "\n",
    "            if episode % evaluate_every == 0:\n",
    "                avg_return = sum(returns[-5:]) / 5\n",
    "                print(f\"Episode: {episode}, Average Return: {avg_return}\")\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        valid_actions = [i for i, val in enumerate(state.flatten()) if val == 0]\n",
    "        if not valid_actions:\n",
    "            return None  # No valid actions available, return None or handle as needed\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            return max(valid_actions, key=lambda a: self.get_state_value(state))\n",
    "\n",
    "\n",
    "    def get_state_value(self, state):\n",
    "        state_key = tuple(state.flatten())\n",
    "        return self.state_values.get(state_key, 0)\n",
    "\n",
    "    def update_state_value(self, state, td_error):\n",
    "        state_key = tuple(state.flatten())\n",
    "        self.state_values[state_key] = self.get_state_value(state) + self.alpha * td_error\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        loaded_data = np.load(file_path, allow_pickle=True)\n",
    "        self.state_values = loaded_data['state_values']\n",
    "\n",
    "    def save_data(self, file_path):\n",
    "        np.save(file_path, {'state_values': self.state_values})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for evaluation purposes and for your self checking the code block below after running should:\n",
    "- Initialize the agent and call the train function which trains the agent\n",
    "- Load the stored state value data\n",
    "- Start a single player game of TicTacToe which takes input from the user for moves according to the convention given below, where the trained Q values play as computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Return: 0.0\n",
      "Episode: 500, Average Return: 0.6\n",
      "Episode: 1000, Average Return: 0.6\n",
      "Episode: 1500, Average Return: 0.6\n",
      "Episode: 2000, Average Return: 1.0\n",
      "Episode: 2500, Average Return: 0.8\n",
      "Episode: 3000, Average Return: 1.0\n",
      "Episode: 3500, Average Return: 0.0\n",
      "Episode: 4000, Average Return: 0.6\n",
      "Episode: 4500, Average Return: 0.8\n",
      "Episode: 5000, Average Return: 1.0\n",
      "Episode: 5500, Average Return: 0.6\n",
      "Episode: 6000, Average Return: 1.0\n",
      "Episode: 6500, Average Return: 0.2\n",
      "Episode: 7000, Average Return: 1.0\n",
      "Episode: 7500, Average Return: 0.6\n",
      "Episode: 8000, Average Return: 1.0\n",
      "Episode: 8500, Average Return: 0.0\n",
      "Episode: 9000, Average Return: 0.6\n",
      "Episode: 9500, Average Return: 0.2\n",
      "Episode: 10000, Average Return: 1.0\n",
      "Episode: 10500, Average Return: 0.2\n",
      "Episode: 11000, Average Return: 0.0\n",
      "Episode: 11500, Average Return: 0.8\n",
      "Episode: 12000, Average Return: 1.0\n",
      "Episode: 12500, Average Return: -0.2\n",
      "Episode: 13000, Average Return: 1.0\n",
      "Episode: 13500, Average Return: 0.4\n",
      "Episode: 14000, Average Return: 0.8\n",
      "Episode: 14500, Average Return: 0.2\n",
      "Episode: 15000, Average Return: 0.4\n",
      "Episode: 15500, Average Return: 1.0\n",
      "Episode: 16000, Average Return: -0.6\n",
      "Episode: 16500, Average Return: 0.4\n",
      "Episode: 17000, Average Return: 1.0\n",
      "Episode: 17500, Average Return: 0.8\n",
      "Episode: 18000, Average Return: 1.0\n",
      "Episode: 18500, Average Return: 0.8\n",
      "Episode: 19000, Average Return: 0.2\n",
      "Episode: 19500, Average Return: 1.0\n",
      "Episode: 20000, Average Return: 0.4\n",
      "Episode: 20500, Average Return: 0.4\n",
      "Episode: 21000, Average Return: 0.6\n",
      "Episode: 21500, Average Return: 0.4\n",
      "Episode: 22000, Average Return: 0.6\n",
      "Episode: 22500, Average Return: 1.0\n",
      "Episode: 23000, Average Return: 0.6\n",
      "Episode: 23500, Average Return: 1.0\n",
      "Episode: 24000, Average Return: 1.0\n",
      "Episode: 24500, Average Return: 0.6\n",
      "Episode: 25000, Average Return: 0.2\n",
      "Episode: 25500, Average Return: 0.8\n",
      "Episode: 26000, Average Return: 0.6\n",
      "Episode: 26500, Average Return: 1.0\n",
      "Episode: 27000, Average Return: 1.0\n",
      "Episode: 27500, Average Return: 0.6\n",
      "Episode: 28000, Average Return: 0.8\n",
      "Episode: 28500, Average Return: 0.6\n",
      "Episode: 29000, Average Return: 0.2\n",
      "Episode: 29500, Average Return: 0.6\n",
      "Episode: 30000, Average Return: 0.6\n",
      "Episode: 30500, Average Return: 0.8\n",
      "Episode: 31000, Average Return: 1.0\n",
      "Episode: 31500, Average Return: 1.0\n",
      "Episode: 32000, Average Return: 0.6\n",
      "Episode: 32500, Average Return: 1.0\n",
      "Episode: 33000, Average Return: 0.6\n",
      "Episode: 33500, Average Return: 0.6\n",
      "Episode: 34000, Average Return: 0.6\n",
      "Episode: 34500, Average Return: 1.0\n",
      "Episode: 35000, Average Return: 0.6\n",
      "Episode: 35500, Average Return: 0.6\n",
      "Episode: 36000, Average Return: 1.0\n",
      "Episode: 36500, Average Return: 0.2\n",
      "Episode: 37000, Average Return: 0.6\n",
      "Episode: 37500, Average Return: 1.0\n",
      "Episode: 38000, Average Return: 1.0\n",
      "Episode: 38500, Average Return: 0.6\n",
      "Episode: 39000, Average Return: 1.0\n",
      "Episode: 39500, Average Return: 0.8\n",
      "Episode: 40000, Average Return: 1.0\n",
      "Episode: 40500, Average Return: 0.0\n",
      "Episode: 41000, Average Return: 1.0\n",
      "Episode: 41500, Average Return: 0.6\n",
      "Episode: 42000, Average Return: 0.4\n",
      "Episode: 42500, Average Return: 1.0\n",
      "Episode: 43000, Average Return: 0.6\n",
      "Episode: 43500, Average Return: 1.0\n",
      "Episode: 44000, Average Return: 0.8\n",
      "Episode: 44500, Average Return: 0.6\n",
      "Episode: 45000, Average Return: 0.6\n",
      "Episode: 45500, Average Return: 0.8\n",
      "Episode: 46000, Average Return: 0.0\n",
      "Episode: 46500, Average Return: 1.0\n",
      "Episode: 47000, Average Return: 0.2\n",
      "Episode: 47500, Average Return: 1.0\n",
      "Episode: 48000, Average Return: 0.6\n",
      "Episode: 48500, Average Return: 0.0\n",
      "Episode: 49000, Average Return: 0.6\n",
      "Episode: 49500, Average Return: 1.0\n",
      "Episode: 50000, Average Return: 0.4\n",
      "Episode: 50500, Average Return: 0.8\n",
      "Episode: 51000, Average Return: 0.6\n",
      "Episode: 51500, Average Return: 0.8\n",
      "Episode: 52000, Average Return: 0.8\n",
      "Episode: 52500, Average Return: 1.0\n",
      "Episode: 53000, Average Return: 1.0\n",
      "Episode: 53500, Average Return: 0.6\n",
      "Episode: 54000, Average Return: 0.6\n",
      "Episode: 54500, Average Return: 0.6\n",
      "Episode: 55000, Average Return: 0.6\n",
      "Episode: 55500, Average Return: 0.8\n",
      "Episode: 56000, Average Return: 1.0\n",
      "Episode: 56500, Average Return: 0.6\n",
      "Episode: 57000, Average Return: 0.4\n",
      "Episode: 57500, Average Return: 0.0\n",
      "Episode: 58000, Average Return: 0.8\n",
      "Episode: 58500, Average Return: 1.0\n",
      "Episode: 59000, Average Return: 0.6\n",
      "Episode: 59500, Average Return: 0.6\n",
      "Episode: 60000, Average Return: 0.2\n",
      "Episode: 60500, Average Return: -0.2\n",
      "Episode: 61000, Average Return: 0.8\n",
      "Episode: 61500, Average Return: 0.2\n",
      "Episode: 62000, Average Return: 1.0\n",
      "Episode: 62500, Average Return: 0.8\n",
      "Episode: 63000, Average Return: 1.0\n",
      "Episode: 63500, Average Return: 0.2\n",
      "Episode: 64000, Average Return: 1.0\n",
      "Episode: 64500, Average Return: 0.6\n",
      "Episode: 65000, Average Return: 0.2\n",
      "Episode: 65500, Average Return: 0.6\n",
      "Episode: 66000, Average Return: 0.8\n",
      "Episode: 66500, Average Return: 0.8\n",
      "Episode: 67000, Average Return: 0.8\n",
      "Episode: 67500, Average Return: -0.4\n",
      "Episode: 68000, Average Return: 0.6\n",
      "Episode: 68500, Average Return: 1.0\n",
      "Episode: 69000, Average Return: 0.4\n",
      "Episode: 69500, Average Return: 1.0\n",
      "Episode: 70000, Average Return: 1.0\n",
      "Episode: 70500, Average Return: 1.0\n",
      "Episode: 71000, Average Return: 1.0\n",
      "Episode: 71500, Average Return: 1.0\n",
      "Episode: 72000, Average Return: 0.8\n",
      "Episode: 72500, Average Return: 0.4\n",
      "Episode: 73000, Average Return: 0.8\n",
      "Episode: 73500, Average Return: 0.6\n",
      "Episode: 74000, Average Return: 0.8\n",
      "Episode: 74500, Average Return: 0.6\n",
      "Episode: 75000, Average Return: 0.6\n",
      "Episode: 75500, Average Return: 1.0\n",
      "Episode: 76000, Average Return: 1.0\n",
      "Episode: 76500, Average Return: 1.0\n",
      "Episode: 77000, Average Return: 0.6\n",
      "Episode: 77500, Average Return: 1.0\n",
      "Episode: 78000, Average Return: 0.6\n",
      "Episode: 78500, Average Return: 1.0\n",
      "Episode: 79000, Average Return: 0.6\n",
      "Episode: 79500, Average Return: 0.6\n",
      "Episode: 80000, Average Return: 0.2\n",
      "Episode: 80500, Average Return: 1.0\n",
      "Episode: 81000, Average Return: 0.2\n",
      "Episode: 81500, Average Return: 0.2\n",
      "Episode: 82000, Average Return: 0.6\n",
      "Episode: 82500, Average Return: 0.6\n",
      "Episode: 83000, Average Return: 0.8\n",
      "Episode: 83500, Average Return: 0.2\n",
      "Episode: 84000, Average Return: 0.4\n",
      "Episode: 84500, Average Return: 0.6\n",
      "Episode: 85000, Average Return: 1.0\n",
      "Episode: 85500, Average Return: 0.4\n",
      "Episode: 86000, Average Return: 0.6\n",
      "Episode: 86500, Average Return: 0.6\n",
      "Episode: 87000, Average Return: 1.0\n",
      "Episode: 87500, Average Return: 1.0\n",
      "Episode: 88000, Average Return: 0.4\n",
      "Episode: 88500, Average Return: 0.4\n",
      "Episode: 89000, Average Return: 0.4\n",
      "Episode: 89500, Average Return: 0.6\n",
      "Episode: 90000, Average Return: 1.0\n",
      "Episode: 90500, Average Return: 1.0\n",
      "Episode: 91000, Average Return: 1.0\n",
      "Episode: 91500, Average Return: 0.8\n",
      "Episode: 92000, Average Return: 0.6\n",
      "Episode: 92500, Average Return: 0.8\n",
      "Episode: 93000, Average Return: 1.0\n",
      "Episode: 93500, Average Return: 0.6\n",
      "Episode: 94000, Average Return: 0.6\n",
      "Episode: 94500, Average Return: 1.0\n",
      "Episode: 95000, Average Return: 1.0\n",
      "Episode: 95500, Average Return: 1.0\n",
      "Episode: 96000, Average Return: 0.8\n",
      "Episode: 96500, Average Return: 0.6\n",
      "Episode: 97000, Average Return: 0.4\n",
      "Episode: 97500, Average Return: 0.8\n",
      "Episode: 98000, Average Return: 0.6\n",
      "Episode: 98500, Average Return: 0.4\n",
      "Episode: 99000, Average Return: 0.4\n",
      "Episode: 99500, Average Return: 1.0\n",
      "Welcome to Tic-Tac-Toe!\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X | X | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X | X | O\n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X | X | O\n",
      "---------\n",
      "X |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m agent \u001b[39m=\u001b[39m TicTacToeAgent(tic_tac_toe\u001b[39m=\u001b[39mTicTacToe())\n\u001b[1;32m     51\u001b[0m game \u001b[39m=\u001b[39m TicTacToeGame(agent)\n\u001b[0;32m---> 52\u001b[0m game\u001b[39m.\u001b[39;49mplay_game()\n",
      "Cell \u001b[0;32mIn[48], line 31\u001b[0m, in \u001b[0;36mTicTacToeGame.play_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_board()\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m current_player \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mEnter your move (1-9): \u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mchoose_action(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtic_tac_toe\u001b[39m.\u001b[39mgrid)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You will be asked to enter number corresponding to the boards where you want to make your move, for example in 1 3x3 TicTacToe:\n",
    "1 | 2 | 3\n",
    "4 | 5 | 6\n",
    "7 | 8 | 9\n",
    "The model should train a 3x3 TicTacToe by default, you can definitely modify the values(of N, number of iterations etc) for your convenience but training model for bigger N might take lot of time\n",
    "'''\n",
    "\n",
    "tic_tac_toe = TicTacToe(n = 3)\n",
    "agent = TicTacToeAgent(tic_tac_toe=tic_tac_toe)\n",
    "returns = agent.train()\n",
    "agent.save_data('stored_data.npy')\n",
    "\n",
    "class TicTacToeGame:\n",
    "    def __init__(self, agent):\n",
    "        self.tic_tac_toe = TicTacToe()  # Initialize a TicTacToe game instance\n",
    "        self.agent = agent\n",
    "\n",
    "    def print_board(self):\n",
    "        board = self.tic_tac_toe.grid\n",
    "        for row in board:\n",
    "            print(\" | \".join([\"X\" if cell == 1 else \"O\" if cell == -1 else \" \" for cell in row]))\n",
    "            print(\"-\" * 9)\n",
    "\n",
    "    def play_game(self):\n",
    "        print(\"Welcome to Tic-Tac-Toe!\")\n",
    "        current_player = -1\n",
    "        while True:\n",
    "            self.print_board()\n",
    "            if current_player == 1:\n",
    "                action = int(input(\"Enter your move (1-9): \")) - 1\n",
    "            else:\n",
    "                action = self.agent.choose_action(self.tic_tac_toe.grid)\n",
    "\n",
    "            state, reward, done = self.tic_tac_toe.act(action)\n",
    "            if done:\n",
    "                self.print_board()\n",
    "                if reward == 1:\n",
    "                    print(\"Congratulations! You win!\")\n",
    "                elif reward == -1:\n",
    "                    print(\"Sorry, you lost!\")\n",
    "                else:\n",
    "                    print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "            current_player = -current_player  # Switch between players\n",
    "\n",
    "\n",
    "# Assuming 'agent' is already trained and initialized\n",
    "agent = TicTacToeAgent(tic_tac_toe=TicTacToe())\n",
    "game = TicTacToeGame(agent)\n",
    "game.play_game()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
